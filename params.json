{
  "name": "Startup-programming",
  "tagline": "StartUp-Programming project CSC 485E + SENG 480B, 2016, Fall Semester",
  "body": "#_SignTalker_\r\n\r\n##Idea Proposal\r\n\r\nWe are proposing a mobile app that will act as a tool to enable those in the deaf community to have a more dynamic communications with non-deaf people.  For those who are able to lip-read but are not capable of speech are limited to using text-to-speech functions.  This is a disengagement as the listener must wait for the finish typing out a sentence(s) on a mobile keyboard.  Our proposed solution is to use the camera on an Android device and existing gesture recognition and text-to-speech technologies to automatically translate the American Sign Language (ASL) alphabet into speech.\r\n\r\nThe user would place their hand a comfortable distance above the phone (for example, while the device lays flat on a table) with the app open.  Prior to that, they will have performed a set of calibration tests to customize the performance of the app to their hand.  They will fingerspell like normal and tap the phone screen once they finish a word.  The interpreted word is then fed through the Google Text Prediction API to remove false-positive results and feed to a text-to-speech API.  Thus, allowing the user to to communicate verbally in near real-time.\r\n\r\n## Authors and Contributors\r\n1. Amanda Dash [adash42@uvic.ca]\r\n2. Nora Huang [norah@uvic.ca]\r\n3. Dany Cabrera [dcabrera@uvic.ca]\r\n4. Tristan Partridge [tpart526@uvic.ca]\r\n5. Maria Ferman [mfermang@uvic.ca]\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}