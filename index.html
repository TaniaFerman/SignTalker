<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Startup-programming by TaniaFerman</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Startup-programming</h1>
        <p>StartUp-Programming project CSC 485E + SENG 480B, 2016, Fall Semester</p>

        <p class="view"><a href="https://github.com/TaniaFerman/StartUp-Programming">View the Project on GitHub <small>TaniaFerman/StartUp-Programming</small></a></p>


        <ul>
          <li><a href="https://github.com/TaniaFerman/StartUp-Programming/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/TaniaFerman/StartUp-Programming/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/TaniaFerman/StartUp-Programming">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
          <a id="signtalker" class="anchor" href="#signtalker" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>SignTalker</em>
          <img src="Users/MariaFerman/Desktop/SUP/signTalker">
        </h1>

<h2>
<a id="idea-proposal" class="anchor" href="#idea-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Idea Proposal</h2>

<p>We are proposing a mobile app that will act as a tool to enable those in the deaf community to have a more dynamic communications with non-deaf people.  For those who are able to lip-read but are not capable of speech are limited to using text-to-speech functions.  This is a disengagement as the listener must wait for the finish typing out a sentence(s) on a mobile keyboard.  Our proposed solution is to use the camera on an Android device and existing gesture recognition and text-to-speech technologies to automatically translate the American Sign Language (ASL) alphabet into speech.</p>

<p>The user would place their hand a comfortable distance above the phone (for example, while the device lays flat on a table) with the app open.  Prior to that, they will have performed a set of calibration tests to customize the performance of the app to their hand.  They will fingerspell like normal and tap the phone screen once they finish a word.  The interpreted word is then fed through the Google Text Prediction API to remove false-positive results and feed to a text-to-speech API.  Thus, allowing the user to to communicate verbally in near real-time.</p>

<h2>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h2>

<ol>
<li>Amanda Dash [<a href="mailto:adash42@uvic.ca">adash42@uvic.ca</a>]</li>
<li>Nora Huang [<a href="mailto:norah@uvic.ca">norah@uvic.ca</a>]</li>
<li>Dany Cabrera [<a href="mailto:dcabrera@uvic.ca">dcabrera@uvic.ca</a>]</li>
<li>Tristan Partridge [<a href="mailto:tpart526@uvic.ca">tpart526@uvic.ca</a>]</li>
<li>Maria Ferman [<a href="mailto:mfermang@uvic.ca">mfermang@uvic.ca</a>]</li>
</ol>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/TaniaFerman">TaniaFerman</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
